\documentclass[article,shortnames,nojss]{jss}

% Vignette options
%\VignetteEngine{knitr::knitr}
%\VignettePackage{dtwclust}
%\VignetteDepends{TSclust, doParallel}
%\VignetteIndexEntry{Comparing Time-Series Clustering Algorithms in R Using the dtwclust Package}
%\VignetteEncoding{UTF-8}

% Maths
\usepackage{amsmath}

% Captions in floating environments
\usepackage[labelsep=colon,font=small,margin=5pt,format=hang]{caption}

% For subcaptions in figures
\usepackage{subcaption}

% For professional looking tables
\usepackage{booktabs}
%\usepackage{multirow}
%\usepackage{multicol}
%\usepackage{longtable}
%\usepackage{tabularx}

% For \FloatBarrier
\usepackage[section,below]{placeins}

% Linebreaks in tables
\usepackage{makecell}

% References
\usepackage[capitalise]{cleveref}

% Used for knitr images from Rnw files
\newcommand{\subfloat}[2][default for first parameter: need a sub-caption]{\subcaptionbox{#1}{#2}}

% Allowed percentages of page a float can use
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}
\renewcommand{\floatpagefraction}{0.75}
\setcounter{totalnumber}{5}

% Shortcuts
\newcommand{\R}{\proglang{R}}
\newcommand{\dtwclust}{\pkg{dtwclust}}
\newcommand{\dtwp}{\text{DTW}_p}
\newcommand{\kshape}{\textit{k}-Shape}

% ===========================================================================================================
% Cover
% ===========================================================================================================

\title{Comparing Time-Series Clustering Algorithms in \R{} Using the \dtwclust{} Package}
\Plaintitle{Comparing Time-Series Clustering Algorithms in R Using the dtwclust Package}

\author{Alexis~Sard\'a-Espinosa}
\date{\today}

\Address{
Alexis Sard\'a-Espinosa\\
\email{alexis.sarda@gmail.com}
}

\Abstract{
Abstract
}

\Keywords{time-series, clustering, \R{}, dynamic time warping, lower bound}
\Plainkeywords{time-series, clustering, R, dynamic time warping, lower bound}

% ===========================================================================================================
% Main matter
% ===========================================================================================================

\begin{document}

<<setup, include = FALSE, cache = FALSE>>=
library(knitr)
library(dtwclust)

data(uciCT)

# knitr defaults
opts_chunk$set(fig.width = 8, fig.asp = 0.5625,
               out.width = "\\linewidth",
               fig.align = "center", fig.pos = "htbp",
               cache = TRUE, echo = FALSE, autodep = TRUE)
@


\section{Introduction}
\label{sec:introduction}

Cluster analysis is a general task which concerns itself with the creation of groups of objects, where each group is called a cluster. Ideally, all members of the same cluster are similar to each other, but are as dissilimar as possible from objects in a different cluster. There is no single definition of a cluster, and the characteristics of the objects to be clustered varies. Thus, there are several algorithms to perform clustering. Each one defines specific ways of defining what a cluster is, how to measure similarities, how to find groups efficiently, etc. Additionally, each application might have different goals, so a certain clustering algorithm may be preferred depending on the type of clusters sought \citep{kaufman1990}.

Clustering algorithms can be organized differently depending on how they handle the data and how the groups are created. When it comes to static data, i.e. if the values do not change with time \citep{liao2005}, clustering methods are usually divided into five major categories: \textbf{partitioning (or partitional)}, \textbf{hierarchical}, \textbf{density-based}, \textbf{grid-based} and \textbf{model-based} methods \citep{liao2005, rani2012}. They may be used as the main algorithm, as an intermediate step, or as a preprocessing step \citep{aghabozorgi2015}.

Time-series are a common type of dynamic data that naturally arises in many different scenarios, such as stock data, medical data, and machine monitoring, just to name a few \citep{aghabozorgi2015, aggarwal2013}. They pose some challenging issues due to the large size and high dimensionality commonly associated with time-series \citep{aghabozorgi2015}. In this context, dimensionality of a series is related to time, and it can be understood as the length of the series. Additionally, a single time-series object may be constituted of several values that change on the same time scale, in which case they are identified as multivariate time-series.

There are many techniques to modify time-series in order to reduce dimensionality, and they mostly deal with the way time-series are represented. Changing representation can be an important step, not only in time-series clustering, and it constitutes a wide research area on its own (cf. Table 2 in \citet{aghabozorgi2015}). While choice of representation can directly affect clustering, it can be considered as a different step, and as such it will not be discussed further in this paper.

Time-series clustering is a type of clustering algorithm made to handle dynamic data. The most important elements to consider are the \textbf{similarity or distance measure}, the \textbf{prototype extraction function} (if applicable), the \textbf{clustering algorithm itself}, and \textbf{cluster evaluation} \citep{aghabozorgi2015}. In most cases, algorithms developed for time-series clustering take static clustering algorithms and either modify the similarity definition or the prototype extraction function by an appropriate one, or apply a transformation to the series so that static features are obtained \citep{liao2005}. Therefore, the underlying basis for the different clustering procedures remains approximately the same across clustering methods. The most common approaches are hierarchcial and partitional clustering (cf. Table 4 in \citet{aghabozorgi2015}), the latter of which includes fuzzy clustering.

\citet{aghabozorgi2015} classify time-series clustering algorithms based on the way they treat the data and how the underlying grouping is performed. One classification depends on whether the \textbf{whole} series, a \textbf{subsequence}, or individual \textbf{time points} are to be clustered. On the other hand, the clustering itself may be \textbf{shape-based}, \textbf{feature-based} or \textbf{model-based}. \citet{aggarwal2013} makes an additional distinction between online and offline approaches, where the former usually deals with grouping incoming data on-the-go, while the latter deals with data that no longer change.

In this context, it is common to change the distance measure for the \textbf{Dynamic Time Warping} (DTW) distance \citep{aghabozorgi2015}. The calculation of the DTW distance involves a dynamic programming algorithm that tries to find the optimum warping path between two series under certain constraints. However, the DTW algorithm is computationally expensive, both in time and memory utilization. Over the years, several variations and optimizations have been developed in an attempt to accelerate or optimize the calculation. Some of the most common techniques will be discussed in more detail in \cref{sec:dtw}.

Due to its nature, clustering procedures lend themselves for parallelization, since a lot of similar calculations are performed independently of each other. This can make a very significant difference, especially if the data complexity increases (which can happen really quickly in case of time-series), or some of the more computationally expensive algorithms are used.

Variations in the clustering procedure could have a big impact in performance with respecto to cluster quality and execution time. As such, it is desirable to have a common platform on which clustering algorithms can be tested and compared against each other. The \dtwclust{} package, developed for the \R{} statistical software \citep{rcore}, provides such functionality, and includes implementations of recently developed time-series clustering algorithms and optimizations. Many of the included algorithms and optimizations are tailored to the DTW distance, hence the name's package. However, the main clustering function is flexible so that one can test many different clustering approaches, using either the time-series directly, or by applying suitable transformations and then clustering in the resulting space. Instead of providing a detailed explanation of the existing algorithms, we will concern ourselves with describing which ones are available in \dtwclust{}, mentioning the most important characteristics of each and showing how the package can be used to evaluate them. Additionally, the variations related to DTW and other common distances will be explored, and the parallelization strategies and optimizations will be described. Refer to \cref{app:notes} for some technical notes about the package. For a more comprehensive overview of the state of the art in time-series clustering, the reader is referred to the included references and the articles mentioned therein.

The rest of this paper is organized as follows. The relevant information to the distance measures will be presented in \cref{sec:distances}. Supported algorithms for prototype extraction will be discussed in \cref{sec:prototypes}. The main clustering algorithms will be described in \cref{sec:clustering}. Some basic information with respect to cluster evaluation will be provided in \cref{sec:evaluation}, and the final remarks will be given in \cref{sec:conclusion}. Code examples will be given in the appendices. The data used throughout this paper is included in the package (saved in a list called \code{CharTraj}), and is a subset of the character trajectories dataset found in \citet{lichman2013}; they are pen tip trajectories recorded for individual characters, and the subset contains 5 examples of the $X$ velocity for each considered character.

\section{Distance measures}
\label{sec:distances}

Calculating distances, as well as cross-distance matrices, between time-series objects is one of the cornerstones of any clustering algorithm. It is a task that is repeated very often and loops across all objects applying a suitable distance function. The \pkg{proxy} package \citep{proxy}, also developed for \R{}, provides a highly optimized and extensible framework for these calculations, and is used extensively by \dtwclust{}. It includes several common metrics, which are saved in a database object called \code{pr_DB}. Additionally, users can register custom functions in the database via \code{pr_DB$set_entry}. This has the advantage that all registered functions can be used with the \code{proxy::dist} function, which results in a high level of consistency. Refer to \cref{app:custom-dist} for an example. Unless otherwise noted, all the distances discussed here are registered with \pkg{proxy} when \dtwclust{} is attached.

It is important to note that the \code{proxy::dist} function parses all matrix-like objects \textbf{row-wise}, meaning that, in the context of time-series clustering, it would consider the rows of a \textit{matrix} or \textit{data frame} as the time-series. Matrices and data frames cannot contain series with different length, something that can be circumvented by encapsulating the series in a \textit{list}, each element of the list being a single series. Internally, \dtwclust{} coerces all provided data to a list, however, it parses \textbf{matrices row-wise} and \textbf{data frames column-wise}. Moreover, a single multivariate time-series should be provided as a matrix where time spans the rows and the variables span the columns. Thus, several multivariate time-series should be provided as a \textit{list of matrices} to ensure that they are correctly detected, although not all distance functions support multivariate series.

The $l_1$ and $l_2$ vector norms, also known as Manhattan and Euclidean distances respectively, are the most commonly used distance measures, and are arguably the only competitive distances when measuring dissimilarity \citep{aggarwal2001, lemire2009}. They can be efficiently computed, but are only defined for series of equal length and are sensitive to noise and time shifts.

To facilitate notation in the following sections, we define a time-series as a vector (or set of vectors in case of multivariate series) $x$. Each vector has length $n$. In general, $x^v_i$ represents the $i$-th element of the $v$-th variable of the (possibly multivariate) time-series $x$. We will assume that all elements are equally spaced in time in order to avoid the time index explicitly.

\subsection{Dynamic time warping distance}
\label{sec:dtw}

At the core, DTW is a dynamic programming algorithm that compares two series and tries to find the optimum warping curve between them under certain constraints, such as monotonicity. It started being used by the data mining community to overcome some of the limitations associated with the Euclidean distance \citep{keogh2004, berndt1994}. In \dtwclust{}, all DTW calculations are performed by the \pkg{dtw} package \citep{giorgino2009}, which comprehensively aggregates most of the related variations and optimizations.

The easiest way to get an intuition of what DTW does is graphically. \Cref{fig:dtw-intuition} shows the alignment between two sample time-series $x$ and $y$. In this instance, the initial and final points of the series must match, but other points may be warped in time in order to find better matches.

<<dtw-intuition, out.width = "0.75\\linewidth", fig.cap = "Sample alignment performed by the DTW algorithm between two series. The dashed gray lines indicate which points are mapped to each other, which shows how they can be warped in time. Note the vertical position of each series was artificially altered for visualization.">>=
dtw_example <- dtw(CharTraj[[1L]], CharTraj[[2L]], keep.internals = TRUE)
plot(dtw_example, type = "two", offset = 1, match.indices = 30,
     xlab = "Time", ylab = "Series")
@

DTW is computationally expensive. If $x$ has lengh $n$ and $y$ has length $m$, the DTW distance between them can be computed in $O(nm)$ time, which is almost quadratic if $m$ and $n$ are similar. Additionally, DTW is prone to implementation bias since its calculations are not easily vectorized and tend to be very slow in non-compiled programming languages. The \pkg{dtw} package includes a C implementation of the dynamic programming step of the algorithm, which should be very fast; its level of generality may sacrifice some performance, but in most situations it will be negligible. DTW is considered a more robust distance measure for time-series \citep{wang2013}, and can potentially deal with series of different length directly. This is not necessarily an advantage, as it has been shown before that performing linear reinterpolation to obtain equal length may be appropriate if $m$ and $n$ do not vary significantly \citep{keogh2004}. For a more detailed explanation of the DTW algorithm, see e.g. \citet{giorgino2009}. However, there are some aspects that are worth discussing here.

The first step in DTW involves creating a \textbf{local cost matrix} (LCM  or $lcm$), which has $n \times m$ dimensions. Such matrix must be created for every pair of distances compared, meaning that memory requirements may grow quickly as the dataset size grows. Considering $x$ and $y$ as the input series, for each element $(i,j)$ of the LCM, the $l_p$ norm between $x_i$ and $y_j$ must be computed. This is defined in \cref{eq:lcm}, explicitly denoting that multivariate series are supported as long as they have the same number of variables (note that for univariate series, the LCM will be identical regardless of the norm used). Thus, it makes sense to speak of a $\dtwp{}$ distance, where $p$ corresponds to the $l_p$ norm that was used to construct the LCM. However, this norm also plays an important role in the next step of DTW.

\begin{equation}
\label{eq:lcm}
lcm(i,j) = \left( \sum_v \lvert x^v_i - y^v_j \rvert ^ p \right) ^ {1/p}
\end{equation}

In the seconds step, the DTW algorithm finds the path that minimizes the alignment between $x$ and $y$ by iteratively stepping through the LCM, starting at $lcm(1,1)$ and finishing at $lcm(n,m)$, and aggregating the cost. At each step, the algorithm finds the direction in which the cost increases the least under the chosen constraints; see \cref{fig:dtw-path} for a visual representation of the path corresponding to \cref{fig:dtw-intuition}. If we define $\phi = \left\{ (1,1), \ldots, (n,m) \right\}$ as the set containing all the points that fall on the optimum path, then the final distance would be computed with \cref{eq:dtwp}.

\begin{equation}
\label{eq:dtwp}
\dtwp{}(x,y) = \left( \sum lcm(k) ^ p \right) ^ {1/p}, \; \forall k \in \phi
\end{equation}

It is clear that the choice of $l_p$ norm comes into play twice during the DTW the algorithm, but why is this crucial? The \pkg{dtw} package also makes internal use of \pkg{proxy} to calculate the LCM, and it allows changing the norm by means of its \code{dist.method} argument. However, as previously mentioned, this only affects the LCM if multivariate series are used, and it is \textbf{not} considered by \pkg{dtw} during the final calculation in step two of the algorithm. For this reason, a special version of $\text{DTW}_2$ is registered with \pkg{proxy} by \dtwclust{} (called simply \code{"DTW2"}), which also uses \pkg{dtw} for the core calculations, but appropriately uses the $l_2$ norm in the second step.

<<dtw-path, fig.cap = "Visual representation of the optimum path found. The big square in the center represents the LCM created for these specific series.">>=
plot(dtw_example, type = "three")
@

The way in which the algorithm traverses through the LCM is primarily dictated by the chosen \textit{step pattern}. The step pattern is a local constraint that determines which directions are allowed when moving ahead in the LCM as the cost is being aggregated. \Cref{fig:step-patterns} depicts two common step patterns and their names in the \pkg{dtw} package. Unfortunately, very few articles from the data mining community specify which pattern they use, although in the author's experience, the \code{symmetric1} pattern seems to be standard. By contrast, the \code{dtw} function uses the \code{symmetric2} pattern by default, but it is simple to modify this by providing the appropriate value in the \code{step.pattern} argument. The choice of step pattern also determines whether the corresponding DTW distance can be normalized or not (which may be important for series with different length). See \citet{giorgino2009} for a complete list of step patterns and which ones can be normalized.

<<step-patterns, out.width = "0.45\\linewidth", fig.width = 4, fig.asp = 1, fig.cap = "Two common step patterns used by DTW when traversing the LCM. At each step, the lines denote the allowed directions that can be taken, as well as the weight associated with each one.", fig.subcap = c("\\code{symmetric1} step pattern", "\\code{symmetric2} step pattern")>>=
plot(symmetric1)
plot(symmetric2)
@

As final comments, it should be noted that the DTW distance does \textbf{not} satisfy the \textit{triangle inequality}, and it is \textbf{not} symmetric unless both series have the same length, i.e. $\dtwp{}(x,y) \neq \dtwp{}(y,x)$ for $n \neq m$. Currently, out of the included distances in \dtwclust{}, only those based on DTW support multivariate series.

\FloatBarrier
\subsubsection{Global DTW constraints}
\label{sec:dtw-window}

One of the possible modifications of DTW is to use global constraints, also known as window constraints. Such constraints limit the area of the LCM that can be reached by the algorithm. There are many types of windows (see e.g. \citet{giorgino2009}), but one of the most common ones is the Sakoe-Chiba window \citep{sakoe1978}, with which an allowed region is created along the diagonal of the LCM (see \cref{fig:dtw-window-plot}). These constraints can marginally speed up the DTW calculation, but they are mainly used to avoid pathological warping. It is common to use a window whose size 10\% of the series' length, although sometimes smaller windows produce even better results \citep{keogh2004}.

<<dtw-window-plot, out.width = "0.6\\linewidth", fig.asp = 1, fig.cap = "Visual representation of the Sakoe-Chiba constraint for DTW. The red elements will not be considered by the algorithm when traversing the LCM.">>=
dtwWindow.plot(sakoeChibaWindow, window.size = 2, reference = 10, query = 10)
@

Strictly speaking, if the series being compared have different length, a constrained path may not exist, since the Sakoe-Chiba band may prevent the end point of the LCM to be reached \citep{giorgino2009}. In these cases, a \textit{slanted band window} may be preferred, since it stays along the diagonal for series with different length and is equivalent to the Sakoe-Chiba window for series with equal length. If a window constraint is used with \dtwclust{}, a slanted band is employed.

It is not possible to know \textit{a priori} what window size, if any, will be best for a specific application, although it is usually agreed that no constraint is a poor choice. For this reason, it is better to perform tests with the data one wants to work with, perhaps taking a subset to avoid excessive running times.

It should be noted that, when reported, window sizes are always integers greater than zero. If we denote this number with $w$, and for the specific case of the slanted band window, the valid region of the LCM will be constituted by all valid $lcm(i,j \pm w)$ for all $(i,j)$ along the diagonal. Thus, $2w + 1$ elements will fall within the window for a given window size $w$. This is the convention followed by \dtwclust{}.

\FloatBarrier
\subsubsection{Lower bounds for DTW}
\label{sec:lbs}

Due to the fact that DTW itself is expensive to compute, \textit{lower bounds} (LBs) for the DTW distance have been developed. These lower bounds guarantee being less than or equal to the corresponding DTW distance. They have been exploited when indexing time-series databases, classification of time-series, clustering, etc. \citep{keogh2005, begum2015}. Out of the existing DTW LBs, the two most effective are termed \code{LB_Keogh} \citep{keogh2005} and \code{LB_Improved} \citep{lemire2009}. The reader is referred to the respective articles for a detailed definition and proof of the LBs, however some important considerations will be further discussed here.

One crucial step when calculating the LBs is the computation of the so-called \textit{envelops}. These envelops \textbf{require} a window constraint, and thus are dependant on both the type and size of the window. Based on these, a \textit{running} minimum and maximum are computed and, respectively, a lower and upper envelop are generated. \Cref{fig:envelop-plot} depicts a sample time-series with its corresponding envelops for a Sakoe-Chiba window of size 15.

<<envelop-plot, out.width = "0.75\\linewidth", fig.cap = "Visual representation of a time-series (shown as a solid black line) and its corresponding envelops based on the Sakoe-Chiba window. The green dashed line represents the upper envelop, while the red dashed line represents the lower envelop.">>=
lbk <- lb_keogh(CharTraj[[1L]], CharTraj[[2L]], window.size = 15)
matplot(cbind(lbk$lower.env, lbk$upper.env),
        type = "l", lty = 2, col = 2:3,
        xlab = "Time", ylab = "Series")
lines(CharTraj[[2L]])
@

In order for the LBs to be worth it, they must be computed in significantly less time than it takes to calculate the DTW distance. \citet{lemire2009} developed a streaming algorithm to calculate the envelops using no more than $3n$ comparisons when using a Sakoe-Chiba window. This algorithm is implemented in \dtwclust{} using the \proglang{C++} programming language, ensuring an efficient calculation.

Each LB can be computed with a specific $l_p$ norm. Therefore, it follows that the $l_p$ norms used for DTW and LB calculations must match, such that $\text{LB}_p \leq \dtwp{}$. Moreover, $\text{LB\_Keogh}_p \leq \text{LB\_Improved}_p \leq \dtwp{}$, meaning that \code{LB_Improved} can provide a tighter LB. However, \code{LB_Keogh} requires the calculation of one set of envelops for every pair of series compared, whereas \code{LB_Improved} must calculate two sets of envelops for every pair of series. If the LBs must be calculated between several time-series, some envelops can be reused when a given series is compared against many others. This optimization is included in the LB functions registered with \pkg{proxy} by \dtwclust{}.

Finally, it must be noted that the LBs are \textbf{only} defined for series of equal length, and similar to DTW, are \textbf{not} symmetric regardless of the $l_p$ norm used to compute them. Also note that the choice of step pattern affects the value of the DTW distance, changing the tightness of a given LB.

\subsection{Shape-based distance}
\label{sec:sbd}

The shape-based distance (SBD) was recently proposed as part of the \kshape{} clustering algorithm \citep{paparrizos2015}; this algorithm will be further discussed in \cref{sec:shape} and \cref{sec:kshape}. SBD is based on \textit{cross-correlation} with coefficient normalization (NCCc), and it is thus sensitive to scale, which is why \citet{paparrizos2015} recommend \textit{z}-normalization.

This distance can be efficiently computed by utilizing the Fast Fourier Transform (FFT), although that might make it sensitive to numerical precision, which can produce discrepancies between systems with 32 and 64-bit architectures. Nevertheless, it can be very fast, it was very competitive in the experiments performed in \citet{paparrizos2015}, and it supports series with different length directly. Additionally, some FFTs can be reused when computing the SBD between several series. This optimization is also included in the SBD function registered with \pkg{proxy}.

\section{Time-series prototypes}
\label{sec:prototypes}

A very important element of time-series clustering has to do with the calculation of so-called time-series \textit{prototypes}. It is expected that all series within a cluster are similar to each other, and one may be interested in trying to define a time-series that effectively summarizes the most important characteristics of all series in a given cluster; this series is sometimes referred to as an \textit{average} series, and prototyping is also sometimes called time-series averaging, however we will prefer the term ``prototyping'' in this paper. The prototypes could be used for visualization aid or to perform time-series classification, but in the context of clustering, partitional procedures rely heavily on the prototyping function, since the resulting prototypes are used as cluster \textit{centroids}; more information will be provided in \cref{sec:clustering}.

The choice of prototyping function is closely related to the chosen distance measure, and in a similar fashion, it is not simple to know which kind of prototype will be better \textit{a priori}. There are several strategies available for time-series prototyping, although due to their high dimensionality, it is debatable what exactly constitutes an average time-series, and some notions could worsen performance significantly. The following sections will briefly describe some of the common approaches, which are the ones included by default in \dtwclust{}. Nevertheless, it is possible to create custom prototyping functions and provide them in the \code{centroid} argument.

\subsection{Mean and median}
\label{sec:mean-median}

The arithmetic mean is used very often in conjunction with the Euclidean distance, and in many applications, this combination is very competitive, even with multi-variate data. However, because of the structure of time-series, the mean is arguably a poor prototyping choice, and could even perturb convergence of a clustering algorithm \citep{petitjean2011}.

Mathematically, the mean simply takes the average of each time-point $i$ across all variables of the considered time-series. For a cluster $C$ of size $N$, the (possibly multivariate) time-series mean $\mu$ can be calculated with \cref{eq:ts-mean}, where $x^v_{c,i}$ is the $i$-th element of the $v$-th variable from the $c$-th series that belongs to cluster $C$.

\begin{equation}
\label{eq:ts-mean}
\mu^v_i = \frac{1}{N} \sum_c x^v_{c,i}, \; \forall c \in C
\end{equation}

Following this idea, it is also possible to use the median value across series in $C$ instead of the mean, although we are not aware if this has been used in the existing literature. Also note that this prototype is limited to series with equal length and equal amount of variables.

\subsection{Partition around medoids}
\label{sec:pam}

Another very common approach is to use partinion around medoids (PAM). A \textit{medoid} is simply a representative object from a cluster, in this case also a time-series, whose average distance to all other objects in the same cluster is minimal. Since the medoid object is always an element of the original data, PAM is sometimes preferred over mean or median so that the time-series structure is not altered.

Another possible advantage of PAM is that, since the data does not change, it is possible to \textit{precompute} the whole distance matrix once and re-use it on each iteration, and even across different number of clusters. While this is not necessary, it usually saves time overall. This precomputation is done by default by \dtwclust{}.

\subsection{DTW barycenter averaging}
\label{sec:dba}

The DTW distance is used very often when working with time-series, and thus a prototyping function based on DTW has also been developed in \citet{petitjean2011}. The procedure is called DTW barycenter averaging (DBA), and is an iterative, \textit{global} method. The latter means that the order in which the series enter the prototyping function does \textbf{not} affect the outcome.

DBA requires a series to be used as reference (centroid), and it usually begins by \textbf{randomly} selecting one of the series in the data. In each iteration, the DTW alignment between each series in the cluster $C$ and the centroid is computed. Because of the warping performed in DTW, it can be that several time-points from a given time-series map to a single time-point in the centroid series, so for each time-point in the centroid, all the correspoindg values from all series in $C$ are grouped together according to the DTW alignment, and the mean is computed for each centroid point using the values contained in each group. This is then iteratively repeated until a certain number of iterations are reached, or until convergence is assumed.

One way in which DBA can be optimized is in its usage of random access memory (RAM). Since the series used at each iteration do not change, the LCMs generated by DTW always have the same size, so the required memory can be allocated once and re-used on each iteration. In order to ensure that this is the case, \dtwclust{} updates the LCMs when using DBA by means of \proglang{C} code.

DBA is much more computationally expensive, due to all the DTW calculations that must be performed. However, it is very competitive when using the DTW distance, and thanks to DTW itself, it can support series with different length directly, with the caveat that the length of the resulting prototype will be the same as the reference series that was initially chosen by the algorithm.

\subsection{Shape extraction}
\label{sec:shape}

A recently proposed method to calculated time-series prototypes is termed \textit{shape extraction}, and is part of the \kshape{} algorithm (see \cref{sec:kshape}) described in \citet{paparrizos2015}. As with the corresponding SBD (see \cref{sec:sbd}), the algorithm depends on NCCc, and it first uses it to match two series as best as possible. \Cref{fig:sbd-alignment} depicts the alignment that is performed using two sample series.

<<sbd-alignment, out.width = "0.45\\linewidth", fig.width = 6, fig.cap = "Visualization of the NCCc-based alignment performed on two sample series. After alignment, the second (red) series is either truncated and/or prepended/appended with zeros so that its length matches the first(black) series.", fig.subcap = c("Series before alignment", "Series after alignment")>>=
matplot(cbind(CharTraj[[61L]], CharTraj[[65L]]),
        type = "l", lty = 1L,
        xlab = "Time", ylab = "Series")

sbd_align <- SBD(CharTraj[[61L]], CharTraj[[65L]])

matplot(cbind(CharTraj[[61L]], sbd_align$yshift),
        type = "l", lty = 1L,
        xlab = "Time", ylab = "Series")
@

As with DBA, a centroid series is needed, so one is usually \textbf{randomly} chosen from the data. The alignment can be done between series with different length, and since one of the series is \textit{shifted} in time, it may be necessary to truncate and prepend or append zeros to the non-reference series, so that the final length matches that of the reference. This is because the final step of the algorithm builds a matrix with the matched series and performs a so-called maximization of Rayleigh Quotient to obtain the final prototype; see \citet{paparrizos2015} for more details.

The output series of the algorithm \textbf{must} be \textit{z}-normalized. Thus, the input series as well as the reference series \textbf{must} also have this normalization. Even though the alignment can be done between series with different length, it has the same caveat as DBA, namely that the length of the resulting prototype will depend on the length of the chosen reference. Technically, for multivariate series, the shape extraction algorithm could be applied for each variable $v$ of all involved series, but this was \textbf{not} explored by the authors of \kshape{}.

\subsection{Fuzzy-based prototypes}
\label{sec:fuzzy-cent}

Even though fuzzy clustering will not be discussed until \cref{sec:fuzzy}, it is worth mentioning here that its most common implementation uses its own centroid function, which works like a \textit{weighted average}. Therefore, it will only work with data of equal dimension, and it may not be suitable to use directly with raw time-series data.

\section{Time-series clustering algorithms}
\label{sec:clustering}

\subsection{Hierarchical clustering}
\label{sec:hc}

\subsection{Partitional clustering}
\label{sec:pc}

\subsubsection{TADPole clustering}
\label{sec:tadpole}

\subsubsection[k-Shape clustering]{\kshape{} clustering}
\label{sec:kshape}

\subsection{Fuzzy clustering}
\label{sec:fuzzy}

\section{Parallel computation}
\label{sec:parallel}

Using parallelization is not something that is commonly explored explicitly in the literature, but it is something that can be extremely useful in practical applications. In the case of time-series clustering, and particularly when using the DTW distance, parallel computation can result in a very significant reduction in execution times.

There are some important considerations when using parallelization. The amount of \textit{parallel workers}, i.e. subprocesses that can each handle a given task, is dependent on the computer processor that is being used and the number of physical processing cores and logical threads it can handle. Each worker may require additional RAM, and since \R{} can only work with data that is loaded on RAM, so there is no way around this limitation. Finally, the overhead introduced for the orchestration of the parallel workers may be too large when compared to the amount of time each worker needs to finish their assigned tasks, which is especially true for relatively simple workloads. Therefore, using parallelization does \textbf{not} guarantee faster execution times, and should be tested in the context of a specific application.

Handling parallelization has been greatly simplified in \R{} by different software packages. The implementations done in \dtwclust{} use the \pkg{foreach} package \citep{foreach}, since it provides a flexible way of specifying the parallel backend that should be used, and can be extended by other packages. See \cref{app:doParallel} for a specific example.

Before describing the different cases where \dtwclust{} can take advantage of parallel computation, it should also be noted that, by default, there is only one level of parallelization. This means that all tasks performed by a given parallel worker are done sequentially, and they cannot take advantage of further parallelization even if there are workers available.

When performing partitional clustering, it is common to do many repetitions with different random seeds to account for different starting points. When many repetitions are specified directly to \dtwclust{}, the package attempts to assign each repetition to a different worker. This is also the case when the function is called with several values of \code{k}, i.e. when different number of clusters are to be tested; they are detected in partitional, fuzzy and TADPole clustering.

Calculating distance matrices involves performing several independent pairwise comparisons. The default distance function included with \dtwclust{} parallelizes said calculations by grouping the amount of comparisons in \textit{chunks} and assigning a chunk to each parallel worker. For instance, if 100 distance calculations are needed, and there are 4 workers available, each of them would get a chunk with 25 calculations. This should provide a good load balance, since each distance calculation is usually very fast. This \textit{chunking} is also performed by the distance functions used by \dtwclust{} which are registered with \pkg{proxy}, especifically \code{LB_Keogh}, \code{LB_Improved}, \code{SBD} and \code{dtw_lb}.

The included implementation of the TADPole and DBA algorithms also try to create chunks when calculating DTW distances. The amount of calculations needed should be relatively small, so parallelization may not always be worth it in that case.

Finally, in the context of partitional clustering, calculating time-series prototypes is something that is done many times each iteration. Since clusters are mutually exclusive, the prototype calculations can be done in parallel, but this is only worth it if the calculations are time consuming. Therefore, in \dtwclust{}, only DBA and shape extraction attempt to do parallelization when used in partitional clustering.

\section{Cluster evaluation}
\label{sec:evaluation}

Clustering is commonly considered to be an unsupervised procedure, so evaluating its performance can be rather subjective. However, a great amount of effort has been invested in trying to standardize cluster evaluation metrics by using \textit{cluster validity indices} (CVIs). Many indices have been developed over the years, and they form a research area of their own, but there are some overall details that are worth mentioning. The discussion here is based on \citet{arbelaitz2013}, which provides a much more comprehensive overview.

CVIs can be classified as \textbf{internal}, \textbf{external} or \textbf{relative} depending on how they are computed. Focusing on the first two, the crucial difference is that internal CVIs only consider the partioned data and try to define a measure of cluster purity, whereas external CVIs compare the obtained partition to the correct one. Thus, external CVIs can only be used if the \textit{ground truth} is known. Each index defines their range of values and whether they are to be minimized or maximized. In many cases, these CVIs can be used to evaluate the result of a clustering algorithm regardless of how the clustering works internally, or how the partition came to be. The Silhouette index is an example of an internal CVI, whereas the Rand index (and its adjusted version) is an external CVI; these are included with \dtwclust{} as examples of CVIs, although other indices can be implemented by the user.

Knowing which CVI will work best cannot be determined \textit{a priori}, so they should be tested for each specific application. Usually, many CVIs are utilized and compared to each other, maybe using a majority vote to decide on a final result. Furthermore, it should be noted that many CVIs perform additional distance calculations when being computed, which can be very considerable if using DTW. For example, the Silhouette index effectively needs the whole distance matrix to be calculated, making it prohibitive in some cases.

\section{Conclusion}
\label{sec:conclusion}

% ===========================================================================================================
% Bibliography
% ===========================================================================================================

\nocite{*}
\bibliography{REFERENCES}

% ===========================================================================================================
% Appendices
% ===========================================================================================================

\appendix

\section{Technical notes}
\label{app:notes}

The main clustering function in \dtwclust{} has the same name, i.e. \code{dtwclust}. It returns objects of type \code{S4}, which are one way in which \R{} implements classes. The formal elements of the class are called \code{slots}, and can be accessed with the \code{@} operator (instead of the usual \code{$}). All documented information about the slots can be found by typing \code{?"dtwclust-class"} in the console. The associated methods can be found with \code{?"dtwclust-methods"}. Note that the \code{dtwclust} class contains the \code{S3} class \code{hclust} as a superclass. This can lead to some unexpected behavior, such as \code{is.list(new("dtwclust"))} being \code{TRUE}.

Many control parameters can be modified by using the \code{control} parameter, which should have a \code{dtwclustControl} class. A \textbf{named list} is also accepted. The documentation of all control parameters, as well as their default values, can be accessed by typing \code{?"dtwclustControl"} in the console.

The random number generator of \R{} is set to L'Ecuyer-CMRG when \dtwclust{} is attached in an attempt to preserve reproducibility. The user is free to change this afterwards by using the \code{RNGkind} function.

\section[Registering a custom distance with proxy]{Registering a custom distance with \pkg{proxy}}
\label{app:custom-dist}

This example takes the autocorrelation-based distance included in the \pkg{TSclust} package \citep{montero2014} and registers it with \pkg{proxy} so that it can be used either directly, or with \dtwclust{}.

<<example-register-proxy, echo = TRUE, message = FALSE>>=
require(TSclust)

proxy::pr_DB$set_entry(FUN = diss.ACF, names=c("ACFD"),
                       loop = TRUE, type = "metric", distance = TRUE,
                       description = "Autocorrelation-based distance")

# Taking just a subset of the data
# Note that subsetting with single brackets preserves the list format
proxy::dist(CharTraj[3:8], method = "ACFD", upper = TRUE)
@

\section[Using the doParallel package for parallel computation]{Using the \pkg{doParallel} package for parallel computation}
\label{app:doParallel}

One way of using parallelization with \R{} and \pkg{foreach} is by means of the \pkg{doParallel} package \citep{doParallel}. It provides a great level of abstraction so that users can easily configure a parallel backend which can be used by \dtwclust{}. The example below does the backend registration and calls \dtwclust{}, returning to sequential computation after it finishes.

<<example-doParallel, echo = TRUE, message = FALSE>>=
require(doParallel)

# Create parallel workers
workers <- makeCluster(detectCores())

# Preload dtwclust in each worker; not necessary but useful
invisible(clusterEvalQ(workers, library(dtwclust)))

# Register the backend; this step MUST be done
registerDoParallel(workers)

# Calling dtwclust
pc_par <- dtwclust(CharTraj, k = 20L,
                   distance = "dtw", centroid = "dba",
                   seed = 938, control = list(trace = TRUE,
                                              window.size = 15L))

# Going back to sequential computation
stopCluster(workers)
registerDoSEQ()
@

\end{document}
