---
title: "Timing experiments for dtwclust"
author: "Alexis Sarda-Espinosa"
output:
    html_vignette:
        toc: true
        number_sections: true
        fig_width: 7
        fig_height: 7
vignette: >
    %\VignetteEngine{knitr::rmarkdown}
    %\VignettePackage{dtwclust}
    %\VignetteIndexEntry{Timing experiments for dtwclust}
    %\VignetteEncoding{UTF-8}
bibliography: REFERENCES.bib
---

```{r setup, include=FALSE}
library("dtwclust")
data("dtwclustTimings")

dist_single_results <- dtwclustTimings$dist$single
dist_multiple_results <- dtwclustTimings$dist$multiple
cent_results <- dtwclustTimings$cent
clus_tadpole_results <- dtwclustTimings$tadpole
partitional_results <- dtwclustTimings$partitional

factor_columns <- c("series_length", "window_size", "k", "num_repetitions")
adjust_factors <- function(df) {
    for (column in factor_columns) {
        if (column %in% colnames(df)) {
            df[[column]] <- factor(df[[column]])
        }
    }
    df
}

dist_single_results <- adjust_factors(dist_single_results)
dist_multiple_results <- adjust_factors(dist_multiple_results)
cent_results <- adjust_factors(cent_results)
clus_tadpole_results <- adjust_factors(clus_tadpole_results)
partitional_results$dtwlb_vs_dtwbasic$pam <- adjust_factors(partitional_results$dtwlb_vs_dtwbasic$pam)
partitional_results$dtwlb_vs_dtwbasic$pam_vs_reps <- adjust_factors(partitional_results$dtwlb_vs_dtwbasic$pam_vs_reps)
partitional_results$dtwlb_vs_dtwbasic$dba <- adjust_factors(partitional_results$dtwlb_vs_dtwbasic$dba)
partitional_results$sparse_pam_k$non_symmetric <- adjust_factors(partitional_results$sparse_pam_k$non_symmetric)
partitional_results$sparse_pam_k$symmetric <- adjust_factors(partitional_results$sparse_pam_k$symmetric)

# knitr defaults
knitr::opts_chunk$set(echo = FALSE, comment = "#>")
```

# Introduction

Time-series clustering is affected by several factors,
such as the characteristics of time-series themselves,
the choice of distance or centroid function,
etc.
In many situations,
run-time characteristics are more important,
e.g. when the amount of memory is limited by the system,
or when excessive running times must be avoided.
Most of these aspects cannot be generalized,
especially in regards to evaluation of correctness,
but things like complexity or growth rate of an algorithm can be assessed relatively more easily.
To get an idea of the running times that can be expected for some of the algorithms included in `dtwclust`,
a series of timing experiments were made.
Said experiments were not concerned with correctness or accuracy,
and only look at timing characteristics.

These experiments were run using `R` v3.4.1 and `dtwclust` v4.0.3.
The `microbenchmark` package (v1.4-2.1) was also used for most of them.
The computer used was running GNU/Linux (LTS kernel v4.9) with an `i5-6500` Intel processor (4 cores) and 16GB of RAM.
The whole set of experiments took almost 58 hours to complete.
The data used comes from the Character Trajectories set [@lichman2013],
which have different lengths and are originally multivariate series with 3 variables;
the univariate versions were extracted from these.
All scripts are available [online in GitHub](https://github.com/asardaes/dtwclust/tree/master/timing-experiments).
Naturally, since we are dealing with exeuction times,
the experiments cannot be reproduced exactly,
but hopefully the median times would not vary too much between systems with similar characteristics.

# Distance experiments

## Calculations for single time-series

First we look at the results of the timing experiments for single time-series,
i.e., for distances calculated between two individual time-series.
The distances tested are those included in the package.
Here we look at the effect of window sizes and series' lengths.
Each calculation was repeated 100 times and the *median* value was extracted.
An `NA` window size means that no constraint was used,
and note that the vertical scale is different for each facet in the following figure.

```{r dist-single-plot, fig.height=7, fig.cap="*Note the different vertical scales for each facet, even though they are all in microseconds.*"}
ggplot(dist_single_results,
       aes(x = series_length,
           y = median_time_us,
           group = window_size,
           colour = window_size)) +
    geom_line() +
    facet_wrap(~distance, scales = "free_y") +
    theme_bw() +
    theme(legend.position = "bottom")
```

The first interesting result relates to the DTW lower bounds: `lb_keogh` and `lb_improved`.
The window size does not seem to have a very significant effect,
and the running time appears to grow linearly with the series' length for `lb_improved`.
However, `lb_improved` was faster that `lb_keogh`.
Considering that `lb_improved` first needs to calculate `lb_keogh` and then perform additional calculations,
this is somewhat puzzling,
and the reason is not immediately evident.

The shape-based distance also presents weird behavior.
While it was expected that its running time would increase with the series' length,
the bump for the length of 152 is considerably large.
It is true that SBD is based on the FFT,
and thus it adjusts the input series' lengths to powers of 2,
but if that were the cause,
then the bump should have occurred for the series with length of 130,
since the next power of 2 for 109 is 128,
and it jumps to 256 for a length of 130.

DTW and GAK are the only tested distances that support multivariate series.
In the case of DTW,
we can see that a window constraint can indeed have a very significant effect on running time,
considering that a window size of 10 resulted in a calculation that was 4 times faster than when using no constraint.
In this case, using multivariate series (with 3 variables) did not have a very significant effect.

The behavior of GAK was rather surprising.
Its running times increase very fast with the series' length,
and window size seems to have no effect at all.
Adding more variables had a much bigger impact here,
and using series with 3 variables effectively tripled the running time of the calculation.

## Calculations for several time-series

Computing cross-distance matrices for time-series can be optimized in different ways depending on the distance that is used.
In the following sections,
we look at the way the included distances are optimized,
and evaluate the way it affects their running times when doing distance calculations between several time-series.

### Lower bounds

First the results for the lower bounds.
The number of series in `y` is important because the envelopes are calculated for those.
In the case of `dtw_lb`, it means that there are more neighbors too.

```{r dist-multiple-lb-plot, fig.cap="*The facets' columns indicate the number of parallel workers, whereas the rows indicate the distance that was used. The vertical scales are different for each row, but they are all in milliseconds.*"}
id_gg <- grepl("lb", dist_multiple_results$distance)
ggplot(dist_multiple_results[id_gg,],
       aes(x = num_total,
           y = median_time_ms,
           colour = num_y,
           shape = series_length)) +
    geom_point(size = 3) +
    facet_grid(distance ~ num_workers, scales = "free_y") +
    theme_bw() +
    theme(legend.position = "bottom")
```

### Shape-based distance

```{r dist-multiple-sbd-plot, fig.height=5, fig.cap="*The facets' columns indicate the number of parallel workers.*"}
id_gg <- grepl("^sbd$", dist_multiple_results$distance)
ggplot(dist_multiple_results[id_gg,],
       aes(x = num_total,
           y = median_time_ms,
           colour = series_length)) +
    geom_point(size = 3) +
    facet_wrap(~num_workers) +
    theme_bw() +
    theme(legend.position = "bottom")
```

### Dynamic time warping

```{r dist-multiple-dtw-plot, fig.height=8, fig.cap="*The facets' columns indicate the number of parallel workers, whereas the rows indicate the length of the series being considered (all series had the same length for each case). All times are in milliseconds.*"}
id_gg <- grepl("^dtw_[um]", dist_multiple_results$distance)
ggplot(dist_multiple_results[id_gg,],
       aes(x = num_total,
           y = median_time_ms,
           colour = window_size,
           shape = distance)) +
    geom_point(size = 3) +
    scale_shape_manual(values = c(0, 3)) +
    facet_grid(series_length ~ num_workers) +
    theme_bw() +
    theme(legend.position = "bottom")
```

### Triangular global alignment kernel

```{r dist-multiple-gak-plot, fig.height=8, fig.cap="*The facets' columns indicate the number of parallel workers, whereas the rows indicate the length of the series being considered (all series had the same length for each case). All times are in milliseconds.*"}
id_gg <- grepl("^gak_", dist_multiple_results$distance)
ggplot(dist_multiple_results[id_gg,],
       aes(x = num_total,
           y = median_time_ms,
           colour = window_size)) +
    geom_point(size = 3) +
    facet_grid(series_length ~ num_workers) +
    theme_bw() +
    theme(legend.position = "bottom")
```

# Prototyping experiments

## Shape extraction

```{r cent-shape-plot, fig.height=5}
id_gg <- grepl("^shape_", cent_results$cent)
ggplot(cent_results[id_gg,],
       aes(x = num_series,
           y = median_time_ms,
           colour = series_length)) +
    geom_line() +
    facet_wrap(~cent) +
    theme_bw() + 
    theme(legend.position = "bottom")
```

## DTW barycenter averaging

```{r cent-dba-plot, fig.height=8, fig.cap="*The facets' rows indicate the length of the considered series. In the columns, 'byS' means 'by series' and 'byV' means 'by variable' (see DBA documentation).*"}
id_gg <- grepl("^dba_", cent_results$cent)
ggplot(cent_results[id_gg,],
       aes(x = num_series,
           y = median_time_ms,
           colour = window_size)) +
    geom_line() +
    facet_grid(series_length ~ cent) +
    theme_bw() + 
    theme(legend.position = "bottom")
```

# Clustering experiments

With `tsclust`.

## TADPole

No parallel here because it is only parallelized with multiple `dc` values,
and that wasn't tested.

```{r clust-tadpole-plot, fig.cap="*In the facets, lbk stands for lb_keogh and lbi for lb_improved.*"}
ggplot(clus_tadpole_results,
       aes(x = num_series,
           y = median_time_s,
           colour = window_size)) +
    geom_line() +
    facet_wrap(~lb) +
    theme_bw() + 
    theme(legend.position = "bottom")
```

## DTW special cases

### PAM centroids

The different flavors of PAM centroids.
The remarks about sparse matrices apply for other distances too.

```{r clust-part-dtw-pam-plot, fig.height=5}
ggdf <- reshape2::melt(partitional_results$dtwlb_vs_dtwbasic$pam, 
                       id.vars=c("num_series", "k", "window_size"))
ggplot(ggdf,
       aes(x = num_series,
           y = value,
           colour = window_size)) +
    geom_line() +
    facet_wrap(~variable) +
    theme_bw() +
    theme(legend.position = "bottom")
```

```{r clust-part-dtw-pam-reps-plot, fig.height=5}
cols <- setdiff(colnames(partitional_results$dtwlb_vs_dtwbasic$pam_vs_reps),
                "sparse_distmat_filled_percent")
ggdf <- reshape2::melt(partitional_results$dtwlb_vs_dtwbasic$pam_vs_reps[,cols], 
                       id.vars=c("num_series", "k", "num_repetitions"))
ggplot(ggdf,
       aes(x = num_series,
           y = value,
           colour = num_repetitions)) +
    geom_line() +
    facet_wrap(~variable) +
    theme_bw() +
    theme(legend.position = "bottom")
```

```{r clust-part-dtw-pam-reps-distmat-plot, fig.height=5}
ggplot(partitional_results$dtwlb_vs_dtwbasic$pam_vs_reps,
       aes(x = num_series,
           y = sparse_distmat_filled_percent,
           colour = num_repetitions)) +
    geom_line() +
    theme_bw() +
    theme(legend.position = "bottom")
```

### DBA centroids

This probably applies to other non-PAM centroids too.

```{r clust-part-dtw-dba-plot, fig.height=5}
ggdf <- reshape2::melt(partitional_results$dtwlb_vs_dtwbasic$dba, 
                       id.vars=c("num_series", "k", "window_size"))
ggplot(ggdf,
       aes(x = num_series,
           y = value,
           colour = window_size)) +
    geom_line() +
    facet_wrap(~variable) +
    theme_bw() +
    theme(legend.position = "bottom")
```

## Effect of `k` on PAM centroids with sparse matrices

Test different `k` values.
Using non-symmetric DTW and symmetric SBD.

### Non-symmetric

```{r clust-part-sparse-pam-k-nonsymmetric-plot, fig.height=5}
ggdf <- reshape2::melt(partitional_results$sparse_pam_k$non_symmetric, 
                       id.vars=c("num_series", "k"))
ggplot(ggdf,
       aes(x = num_series,
           y = value,
           colour = k)) +
    geom_line() +
    facet_wrap(~variable, scales = "free_y") +
    theme_bw() +
    theme(legend.position = "bottom")
```

### Symmetric

```{r clust-part-sparse-pam-k-symmetric-plot, fig.height=5}
ggdf <- reshape2::melt(partitional_results$sparse_pam_k$symmetric, 
                       id.vars=c("num_series", "k"))
ggplot(ggdf,
       aes(x = num_series,
           y = value,
           colour = k)) +
    geom_line() +
    facet_wrap(~variable, scales = "free_y") +
    theme_bw() +
    theme(legend.position = "bottom")
```

### Remarks

The effect of `k` is small,
and as expected smaller values fill the matrix faster.
But if we consider the results with many repetitions above,
then it doesn't make much sense to use sparse matrices.

# References
